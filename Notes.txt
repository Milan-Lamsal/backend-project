                                            PROJECT SETUP 

                                              A. Basic Setup  

1. When storing images via a third-party service like AWS, Azure, or Cloudinary, the typical approach involves temporarily saving uploaded photos and videos on your server. This ensures that if a user's connection is lost during the upload process, the files are still retained. Once successfully received, the files are then uploaded to Cloudinary (or another cloud provider) using a designated process. Some companies choose to upload files directly to Cloudinary without temporary local storage, depending on their standards and requirements.

2. In Git, folders are not tracked directly—only files are. If you create a folder and add an empty subfolder inside it, Git will not recognize or track those folders unless they contain at least one file. This means an empty subfolder serves no purpose in Git since it won’t be included in version control. If you need Git to track an empty folder, a common practice is to add a placeholder file (like .gitkeep). and it should be under the subfolder

3. .gitignore is used to exclude sensitive files and other unnecessary files from being tracked by Git. This includes API keys, configuration files, environment variables, and other confidential data that should not be pushed to a remote repository for security reasons (.gitignore generator-https://mrkandreev.name/snippets/gitignore-generator/#Node)-Using a .gitignore generator saves time and ensures you don’t accidentally commit unnecessary or sensitive files.Different technologies (like Node.js, Python, or React) have their own sets of files that should be ignored, such as: Node.js projects: node_modules/, .env, .DS_Store, and dist/. , React projects: build/, .env, and .next/ (for Next.js).

4. .env files are used to store environment variables, such as API keys, database credentials, and other sensitive information. When deploying to production, these environment variables should not be stored in the source code but should be set up in the server or cloud provider (e.g., AWS, DigitalOcean). This ensures security by keeping sensitive data outside the codebase.

Modern version control platforms like GitHub now warn users if they attempt to push .env files, helping prevent accidental exposure of sensitive information."

5. src/ is a common directory used to store source files in a project. If you are using Git Bash( Best if you’re familiar with Linux/macOS commands and working with Git.) and run the following command inside your project folder:
cd src && touch app.js constants.js index.js
This will navigate to the src/ directory (if it exists) and create three empty files: app.js, constants.js, and index.js."

6. Package.json -> How to open using script also in JavaScript there is two kind of syntax for import one is(CommonJS (CJS)) and ES Modules (ESM), for this project gonna use Module so that there will be consistent for that "type": "module" 
whenever you change the file the server need to be start or stop for reloading or seeing that changes, so to handle that (newly update in Node js --watch to handle that problem) but in the production grade it hasn't been that used so people use the utiliy called (nodemon)-https://www.npmjs.com/package/nodemon (what it does is)- whenever you file is save it restart the server that all it does.

7. Difference in dev dependency( These are packages needed only during the development phase. They are not required in production. These packages are typically used for tasks like testing, building, or development tools. for example nodemon ) and dependency(These are packages that your application needs to run in both development and production environments. They are required for the application to function properly. for example : express, mongoose ) and if its npm i nodemon then it will install in dependency so nned to install nodemon as a development dependency as npm install --save-dev nodemon  or npm i -D nodemon 

8. In the scripts section of package.json, you can create custom commands to run specific tasks. Instead of using the default test command, you can add a dev command to automatically restart your server using nodemon whenever you make changes to your files. we use "dev": "nodemon src/index.js" -> this says nodemon whenever I make some changes in the file then you restart the server after typing [npm start dev]

9. .env and type:module they don't get along so you can't easily import ES6 import 'dotenv/config' you need to use require('dotenv').config() [https://www.npmjs.com/package/dotenv]

====================================================================================================================
                           B. Professional Structure for Project                                                 

10 . Basic command : ls(to list content  ) , cd .. (to go one step back to the parent directory ) cd src (this will move you to src directory if it exist ), mkdir(to make a folder)

11. controllers -> majorly functionality, db -> how to connect database all the logic written here (MongoDB, PostgresSQL), middlewares->(to run the code in between for example to check in the between when you get the request from the server eg you get request  when you were asking to server now using middlewares in middle to get the cookies so that if you are capable of using that infomation ), models , routes-('/', '/instagram') we use separate folder for that cus in production its gonna be huge , utils->(utilities) you need a lots of utilities like file upload is one utility, like for mailing, like for taking and giving tokens those things that are being repeat why not to to create a file folder and use it and that is called utilities,  all the professional grade project you get this Utils 

11. Prettier -> Its a plugin (mostly said it is a VS code extention use that one) but when you are writing professional grade code you don't write alone, you write with other memeber as well so for eg. if somone used semicomma and someone didn't in JS then when you mergin in the git there would be lots of conflict so team need to be in a same phase which spacing are we using two spacing or four spacing (Tap) this communication is needed, also like if one has two spacing and the other have four spacing there would be crazy amount of conflict working in the same file so use Prettier setting per project basic so we install prettier, 

It is also a Dev dependency majorly so we do npm i -D prettier instead of just npm i prettier for dependency and you need to add two three file manaully  one is (.prettierrc) and rc at last 
{
    "sigleQuote": false,
    "bracketSpacing": true,
    "tablWidth": 2,
    "trailingComma": "es5",
    "semi": true
} 

another one is .prettierignore (in which files not to implement prettier) like /.vscode
/node_modules
./dist

 .env
.env
.env. 

====================================================================================================================
                                          How to connect database in MERN 

12. MongoDB Atlas (gives online Database), In MongoDB we use moslty Network Access and Database Access 

13. In MongoDB Atlas you need two things : a. IP address allow so that you can access  and b. correct ID, Password and ofcourse URL  and go to datbase or overview and connect 

14. In professional settings, when you add an Ip address, it typically done for a specific machine where the backend code is hosted. In production-grade settings, you avoid allowing acess from anywhere for security reasons. In some cases, for testing or if we need to check something, we temporarily allow certain Ips for 6hours or 10 hours.

15. Connecting to Dabatabse take the URL from the overview -> connect-> compass  and  remove the / at the end while putting in .env also PORT="Number"

16. Need to have a name of Database so in Constants we kept export const DB_Name - "videotube"

                                            Two approach to connect database
1. One approach is that we put all the code inside the index file because we are going to execute the index file 1st through node . So we can put all the code inside the index file and as soon as my index file loads, the function where we have written all the database connection code will execute immediately.

2.The second approach is to create a folder named "DB" where the database connection function inside it and then import that function into the index file and execute it there.Both approaches have their pros and cons. If you write the code separately, the code will be clean, well distributed and modular, which is better professional approach. But if you want, you can also put everything inside the index file.

17. App.js will be through express and Database Connection will be through mongoose.We also need npm package of .env(dotenv), mongoose and express 
connecting Mongoose with database - mongoose.connect('mongodb://127.0.0.1:27017/myapp');  instead of local host we will give alas URL 


                                                 Database 
                                REMEMEBER TWO THINGS ((Try Catch and async await ))
The first thing is that whenever you try to interact with database, problems can arise. This means you should wrap it in try catch  block which is much better approach. Now whether you use try catch or promises both can handle errors because errors are handled there as well using resolve and reject. So you must use one of the two. 

The second thing is that the database is always in another continent. This means that database interaction take time so using a async await is necessary. It is always a better approach to wrap database interactions in try catch and ensure proper handling with a async await.  

18. Never connect DB in one line, also remember try putting ; in IIFE ;()() for cleaning purpose also  Mongoose give return object

Database server for production, development Testing are Different in each 

dotenvironemnt .env -> As early as possible in your application, import and configure dotenv:This is an environment variable what we need is as soon as our applicaiton load at the same time all the environment variable should be available if it is available in the main file then all will get accesss, so the first file which load which is index.js in the same file we load environment variable

using experimental feautres 
as import 'dotenv/config' in not in used much so what we can do is in the package.json in script what you can do is scripts : "dev": 'nodemon -r dotenv/config --experimental-json-modules src/index.js' why we doing to make it consistent: maybe no need --experimental-json-modules  it ealier version it needed to make consistent.

import dotenv from "dotenv"
import connectDB from "./db";

dotenv.config({
    path: './env'
})

when you you change something in .env you have to restart the server nodemon doesn't help there

=====================================================================================================================
                                        EXPRESS-> https://expressjs.com/en/5x/api.html

19.  Mostly you will be in API reference, and there are two main that you gonna use it which is REQUEST AND RESPONSE

a.Request -
 it s an object that contains all the details of an incoming HTTP request send by a client (browser,Postman, frontend app, etc). It includes infomation like the method(GET, POST,etc), URL, query parameters(?search=express), route parameters (/users/:id), body data (sent in POST requests), and headers (like authentication tokens). The server reads this data to understand what the client wants and responds accordingly. For example, in a login form, req.body helps retrieve the username and password submitted by the user. Without req, the server wouldn’t know what the client is requesting

  req.params ->(whenever the request comes from URL it comes from req.params)where params means parameters  like /users/:id, "/products/:category/:id

 req.body ->req.body holds the data sent by the client in the body of the request, It’s essential for handling data when the client sends data to the server, such as logging in, submitting a form, or uploading a file! 

 req.cookies -> Cookies are small pieces of data that the server can store in the client’s browser and send back with subsequent requests. They are often used for things like authentication (storing login sessions) or remembering user preferences. To work with cookies in Express, you typically use the cookie-parser middleware to parse the cookies and make them available in req.cookies. ->npm i cookies-parser

 CORS -CORS is a security feature that manages cross-origin requests between different domains.CORS ensures that only trusted websites can make requests to your server, preventing malicious activity from untrusted sources!
 npm i cors

 You use app.use(cors()0)// Global Middleware: You use app.use() when you want to apply the middleware to all incoming requests, regardless of the route.

 CORS_ORIGIN =   here star mean from anywhere 

  Middleware
 Middleware in Express is like a helper function that runs between receiving a request and sending a response. It can modify, check, or handle the request before it reaches the final route handler.
Think of middleware as a checkpoint where you can:  Process incoming requests (e.g., check if a user is logged in)

             (err, req, res, next) 
next → Calls the next middleware function in the stack.


b. Responds->
In Express.js, a response (res) is an object that allows the server to send data back to the client after processing a request. It can send plain text, JSON, HTML, status codes, or even redirect the client to another page. For example, res.send("Hello!") sends a simple text response, while res.json({ message: "Success" }) sends JSON data. The response also includes status codes like res.status(404).send("Not Found") to indicate errors. Without res, the client wouldn’t receive any output from the server, making it essential for communication between the backend and frontend! 


20. Utility ->In backend development, the utils (short for utilities) folder is used to store helper functions, reusable modules, and utility scripts that are not tied to a specific route or database model. These functions simplify code, promote reusability, and keep the project organized.


                               To control error 
21 Nodejs api error -https://nodejs.org/api/errors.html#class-error

22. Server Status Code 
Informational responses (100 – 199)
Successful responses (200 – 299)
Redirection messages (300 – 399)
Client error responses (400 – 499)
Server error responses (500 – 599)

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
 23                                           Working with models
                                    
mongoose-aggregate-paginate-v2
https://www.mongodb.com/docs/manual/core/aggregation-pipeline/


bcrpt - A bcrypt library for Nodejs
bcrptjs- Optimized bcrypt in plain js with zero dependencies. Compatible to 'bcrypt' 

bcrypt is a password hashing library used to securely store passwords in a database. Instead of saving raw passwords (which is very unsafe), bcrypt converts passwords into an unreadable format (hash), making them difficult for attackers to crack.

Token key - jsonwebtoken -npm i jsonwebtoken  -Used for secure login, user authorization, and API security. -> 

A Bearer Token in JWT is like a digital key that allows a user to access protected resources (APIs, routes, etc.). It is included in the Authorization header when making requests to the serve
The word "Bearer" means whoever holds this token has access. If someone has your token, they can use it to access protected resources without needing a username & password.


                                Middleware pre hook 
Just before your data save, you can use this pre hook so as to make your password encrypt 

SHA256-HA-256 (Secure Hash Algorithm 256-bit) is a cryptographic hash function that converts input data into a 256-bit (32-byte) fixed-length hash. It is widely used for data integrity, password hashing, digital signatures, and blockchain security.


/////////////////////////////////////////////////////////////////////////////////////////////////////////
24                              Cloudinary - for uploading file and images and many more
                              npm i Cloudinary

Two packages needed for uploading files in the backend 
1. express-fileupload -http://npmjs.com/package/express-fileupload
2. Multer -npm i multer (mosstly used )

 a two-step strategy for file uploads in a production environment: first, temporarily saving the file on the backend server and then uploading it to Cloudinary.
Here's what the transcript says about this process:
•
User Upload via Multer: When a user uploads a file, the backend uses Multer middleware to receive the file and save it temporarily on the local server's file system.
•
Temporary Local Storage: This temporary storage is achieved using Multer's disk storage engine, which allows specifying a destination folder (e.g., public/temp/) on the server to save the incoming file.
•
Rationale for the Two-Step Approach: The primary reason for this two-step process is to enhance resilience and provide an opportunity for retries. If the subsequent upload to Cloudinary fails for any reason, the file is already present on the server, making it possible to attempt the upload again. This is described as a common practice in production-grade server


Cloudinary Upload: After the file is temporarily saved on the server, the backend then uses the Cloudinary SDK to take the local file path and upload the file to the Cloudinary service. The uploadOnCloudinary utility function is designed to handle this step.
•
Local File Removal: After a successful (or even failed) attempt to upload to Cloudinary, the transcript emphasizes the importance of deleting the temporarily saved file from the local server using Node.js's fs.unlinkSync() method. This prevents the accumulation of potentially corrupted or unnecessary files on the server's disk. fs -> file system helps  where you can read write delete and update 

Alternative: Direct Upload: The transcript acknowledges that it is technically possible to directly upload the file from Multer to Cloudinary without the intermediate local storage step. However, it notes that the two-step approach is more common in production settings for the reasons mentioned above

By using multer we gonna creat Middleware - for registration 


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                              Router and controllers
This section marks the start of learning how to write controllers in the back-end journey. Writing more controllers is an exercise for logic building. Logic building involves breaking down large problems into smaller phases and solving them step-by-step

Practical Example: Registering a User
will focus on the task of registering a user as a practical example,Even a seemingly simple task like user registration has many sub-parts

Creating the User Controller

A new file named user.controller.js is created within a controllers folder.
An asyncHandler helper file (assumed to be pre-existing) is imported to handle asynchronous requests and responses, potentially simplifying promise and error handling.
A method named registerUser is created within the controller.

The syntax used in registerUser will be commonly repeated.
registerUser uses the asyncHandler higher-order function, which accepts a function with request and response parameters.The initial implementation of registerUser sends a 200 status code and a JSON response with the message "OK".The registerUser function is exported from user.controller.js

Setting up User Routes

A new folder named routes is created to organize route definitions.A new file named user.routes.js is created for user-related routes.An Express Router is imported and instantiated.The created Router is exported as the default from user.routes.js.Separating routes improves code manageability
ntegrating Routes in app.jsIn app.js, routes are typically imported after other middleware.
The userRouter is imported from user.routes.js. The .js extension is included in the import statement.
app.use() is used to mount the userRouter as middleware under the path /api/v1/users. This prefix includes API versioning as a good practice.
When a request hits /api/v1/users, the userRouter handles it.
Within userRouter in user.routes.js, the route /register is defined using router.route('/register').
The POST method is associated with the /register route using .post(), and the registerUser controller function is specified to handle these requests
Running the application initially resulted in an error: "Router.post requires a callback function".
The debugging process involved systematically checking import statements and function definitions.
The root cause was an issue in the asyncHandler higher-order function, where it was not properly returning the accepted function.
After adding a return statement to the asyncHandler, the server started running.
The speaker emphasized reading error messages carefully and checking potential issues like database connection, credentials, and file imports

Testing the API Endpoint with Postman

Postman is used as the primary tool to test the API endpoint, described as an industry standard. Thunder Client is mentioned as a VS Code plugin alternative.A POST request needs to be sent to http://localhost:8000/api/v1/users/register (adjusting the port if necessary).Sending a GET request by mistake resulted in a "Cannot GET" error, highlighting the importance of matching the HTTP method.Upon sending a POST request, the server responded with a 200 OK status and the JSON message {"message": "Okay"} (later changed to {"message": "Chai aur Code"}).The response details in Postman (status code, headers, body, time taken) are noted.Demonstrations showed how changing the status code in the controller (e.g., to 400 or 500) is reflected in Postman
///////////////////////////////////////////////////////////
1 Logic Building for User Registration:

Logic building for real-world software like registration involves understanding a problem and breaking it down into smaller, manageable steps.Instead of immediately writing code, it's important to define the sequential steps required to solve the problem.

For user registration, the identified steps include:

Get user details from the frontend (e.g., using request.body in a backend framework like Express).

 This could include full name, email, username, and password. Data can come from the request body (JSON) or potentially other sources like URLs or forms.

Validation of user input, such as checking for empty fields (e.g., full name, email, username, password) and potentially the correct format of email. A JavaScript method like .trim() can be used to check for empty strings after removing whitespace. The .some() method on an array of fields can be used to efficiently check if any of the required fields are empty.

Check if user already exists in the database, possibly by querying based on email and/or username to ensure uniqueness. Mongoose's findOne method can be used for this purpose, along with the $or operator to check against multiple fields.

Check for images (avatar and cover image) if the application requires them.

Check for a mandatory avatar image.

Upload images to cloud storage like Cloudinary using a pre-built utility function. This involves taking the local file path provided by middleware like Multer. The await keyword is necessary for asynchronous operations like cloud uploads.

Create a user object with all the necessary details, including the URLs of uploaded images from the cloud service. Handle cases where the cover image might not be provided by setting its URL to empty. Ensure password encryption is awaited.

Create an entry in the database using a method like User.create() provided by an ORM like Mongoose. This is also an asynchronous operation that should be awaited.
Remove sensitive fields like password and refresh token from the response before sending it back to the client. This can be done using the .select() method in Mongoose with a negative sign to exclude fields.

Check for successful user creation by querying the database for the newly created user, possibly by ID.
Return a response indicating success or failure to the front end. A consistent response structure can be achieved using a custom API response class. In case of an error during user creation, a 500 status code might be appropriate.

Middleware for File Handling:

Middleware, like Multer, can be used to handle file uploads. It sits between the route and the controller logic.Multer can be configured with storage options and used to process single or multiple files associated with form fields.

The upload.fields() method in Multer is used to handle multiple files with different field names, accepting an array of objects specifying the name of the field and the maxCount of files to accept for that field.
Once the Multer middleware is applied to a route, the uploaded files can be accessed via request.files. Each file object in request.files contains information about the uploaded file, including its local path after being stored by Multer.

Error Handling:

A custom APIError class can be created to standardize error responses, including a status code and a message. Using throw new APIError() simplifies the process of returning error responses from different parts of the controller logic.Different HTTP status codes can be used to indicate different types of errors (e.g., 400 for bad request, 409 for conflict, 500 for internal server error).Asynchronous operations, especially those involving database interactions or external services, should be wrapped in async functions and use the await keyword to handle promises and potential errors.

Code Structure and Best Practices:Importing necessary modules (like the user model and utility functions) at the beginning of the controller file is crucial.De-structuring the request.body to extract user details can make the code cleaner.Using descriptive variable names improves code readability.While basic validation with multiple if conditions is acceptable for beginners, more experienced developers might use techniques like array methods (some, map) for more concise validation.It's good practice to log data (e.g., console.log(request.files)) during development to understand the structure of objects and responses.Testing the API endpoints with tools like Postman is essential throughout the development process.

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                         How to use Postman for backend
 
Logic Building 

     Real-world projects are key for mature logic building .
    Building features like login and register helps develop logic through feedback and essential requirements.
    Real-world software development involves  dividing big problems into smaller ones .
    Logic building in this context is also referred to as  business logic building .
    The process involves  breaking down a problem into steps  before writing code.

 Register User - The Problem and Steps (Algorithm) 

    The challenge is to  register a user , which involves multiple steps.
     Algorithm/Steps to Register a User: 
         Get user details from front end  (request body).
         Validation:  Check for not empty fields (e.g., username, email) and potentially email format.
         Check if user already exists  (by email and/or username).
         Check for images , specifically the avatar (required) and cover image (optional).
        If images are available,  upload them to Cloudinary  using a utility.
         Successfully check if the avatar was uploaded  to Cloudinary.
         Create user object  with all the necessary details.
         Create entry in DB  using Mongoose's `create` method.
         Remove password and refresh token field from response .
         Check for user creation  (ensure a valid response from the database).
         Return response  with user data (excluding sensitive fields) or an error.

 Getting User Details from Front End 

    User details are typically received in the  request body .
    Data can come from various sources (e.g., URL, forms, JSON), but the focus is on `request.body`.
     Destructuring  can be used to extract specific fields like `full name`, `email`, `username`, and `password` from `request.body`.

 Handling File Uploads (Multer Middleware) 

     Multer middleware  is used to handle file uploads.
    It needs to be imported and used in the  routes  before the controller logic.
    `upload.fields()` method is used to handle  multiple files with different field names  (e.g., 'avatar' and 'coverImage').
    For each field, you can specify the  `name`  (matching the front-end field) and  `maxCount` .
    Multer adds a `request.files` object containing information about the uploaded files.

 Data Validation 

     Check for empty fields  using conditional statements.
    An array with all required fields can be checked using the  `.some()` method  to see if any field, after trimming, is empty.
    Custom `APIError` class is used to return consistent error responses with a status code (e.g., 400 for required fields) and a message.
    Additional validations (e.g., email format) can be implemented with further conditional checks or separate validation functions.

 Checking if User Already Exists 

    Import the  User model  to interact with the database.
    Use the  `User.findOne()` method  to query the database for a user with a matching email or username.
    The  `$or` operator  in MongoDB can be used to check if either the email or the username exists.
    If an existing user is found, an `APIError` with a  409 status code  (conflict) should be thrown.

 Handling Images (Local Path) and Uploading to Cloudinary 

    Multer stores uploaded files locally based on its configuration.
    The  `request.files` object  provides access to the uploaded files, organized by the field name (e.g., `request.files['avatar']`).
    Each uploaded file within `request.files` is typically an array, and the first element contains file information, including the  `path`  to the locally stored file.
    A local path for the avatar (`avatarLocalPath`) and cover image (`coverImageLocalPath`) is retrieved.
    A check is performed to ensure the  `avatarLocalPath` exists  (as it's required), and an error is thrown if it doesn't.
    A pre-built  `uploadOnCloudinary` utility function  is used to upload images to Cloudinary, taking the local file path as input.
    `await` is used when calling `uploadOnCloudinary` as it's an asynchronous operation.
    The response from Cloudinary (containing the URL) is stored.

 Creating User Object and Database Entry 

    A  user object  is created with the collected data, including the Cloudinary URLs for avatar and cover image.
    The  `User.create()` method  is used to save the new user object to the MongoDB database.
    `await` should be used with `User.create()` as it's an asynchronous operation.

 Removing Sensitive Information and Returning Response 

    After successful user creation, the  password and refresh token fields should be removed  from the user object before sending the response.
    The  `select()` method  in Mongoose can be used with a negative sign (`-`) to exclude specific fields from the query result.
    A final check can be done by querying the database  by ID  to ensure the user was created successfully.
    A custom  `APIResponse` class  is used to format the successful response with a status code (e.g., 201 for successful creation), data (the created user object without sensitive fields), and a message.
    If user creation fails at any point after initial data retrieval, an appropriate `APIError` (e.g., 500 for server-side issues) should be thrown.

    //////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                          Postman
      Testing API Endpoints:   Postman is used to send requests to the backend server and observe the responses. In the context of the register controller, a   POST request   is sent to the relevant URL.
      Simulating Front-End Input:   When a front-end is not yet available, Postman can be used to   manually provide the user details   required for registration.
      Sending Data:   The transcript discusses different ways to send data through Postman:
          Request Body:   The focus is on sending data via the   request body  .
          Form Data:   While Postman supports sending form data (with options for text and files), the initial testing focuses on sending   raw data as JSON  . The ability to send files via form data will likely be used later when testing the image upload functionality.
          Raw JSON:   For the initial testing, the instructor demonstrates sending user details like   email in JSON format   within the request body. This involves selecting the "raw" option and choosing "JSON" as the data type in Postman.
          Parameters:   The transcript briefly mentions   parameters   as another way to send data from the front-end, but the focus for this controller is on the request body.
      Observing the Response:   After sending a request, Postman allows you to see the   response from the server  . This includes the   status code   (e.g., 200, 201, 400, 409, 500) and the   response body  , which in this case will eventually contain the registered user data (excluding sensitive fields) or an error message.
      Debugging:   Postman helps in   debugging the backend logic   by allowing developers to see if the server receives the data correctly and if the response is as expected. The example shows how sending an email via Postman confirms that the server can receive and process it (at least to the point of printing it in the console).
      Configuration:   The instructor mentions that the   font in Postman   has been set up, indicating some level of customization of the tool.

// Server is running at port :8000
email: whatever@gmail.com
[Object: null prototype] {
  avatar: [
    {
      fieldname: 'avatar',
      originalname: 'Whileloop.png',
      encoding: '7bit',
      mimetype: 'image/png',
      destination: './public/temp',
      filename: 'Whileloop.png',
      path: 'public\\temp\\Whileloop.png',
      size: 14393
    }
  ],
  coverImage: [
    {
      fieldname: 'coverImage',
      originalname: 'Vote Output.png',
      encoding: '7bit',
      mimetype: 'image/png',
      destination: './public/temp',
      filename: 'Vote Output.png',
      path: 'public\\temp\\Vote Output.png',
      size: 29151
    }
  ]
}

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                               ACCESS TOKEN AND REFRESH TOKEN
In the provided YouTube video transcript, the speaker explains the concepts of access tokens and refresh tokens in the context of backend development. Here are the key points about them:

      Modern Practice  : Using both access and refresh tokens is a   modern security practice   for web applications. However, it's mentioned that not every tutorial uses this approach, and applications can function with just an access token.
      Purpose  : Both access tokens and refresh tokens are used for   authentication  , allowing users to access protected resources.
      Lifespan  : The primary difference between them is their   expiration time  .
          Access tokens are designed to be short-lived  . The definition of "short-lived" is relative and can range from minutes to hours or a day, depending on the application's security needs. They are typically made to expire quickly for security reasons, potentially requiring the user to log in again after a certain period if only the access token was used.
          Refresh tokens are designed to be long-lived  . Similar to access tokens, the duration of "long-lived" is comparative and can vary significantly.
      Usage Scenario  :
        As long as a user has a valid   access token  , they can access features and resources that require authentication.
        When an   access token expires  , instead of forcing the user to log in again with their password, the   refresh token   comes into play.
        The   refresh token is stored in the database   and is also given to the user (e.g., as a cookie).
        The frontend can then send the   refresh token to a specific endpoint   on the server.
        The server then   compares the received refresh token with the one stored in the database   for that user.
        If they match, the server can issue a   new, short-lived access token   to the user, thus renewing their session without requiring them to re-enter their credentials.
      Implementation in the Project  : The user model in the project has functions like `generateAccessToken` and `generateRefreshToken`. During the login process, both types of tokens are generated. The refresh token is saved in the database associated with the user. Both tokens are then typically sent to the client, often as secure HTTP-only cookies. They might also be sent in the JSON response, which can be useful for scenarios like mobile applications where cookie handling might be different.
      Retrieval  : The access token can be retrieved from the request cookies using `request.cookies.accessToken` or from the `Authorization` header (Bearer token).
      Security  : The access token is verified using JWT (JSON Web Tokens) and a secret key. The `verifyJWT` middleware checks the validity of the access token to authenticate the user for protected routes.

In essence, access tokens provide temporary access to resources, while refresh tokens provide a mechanism to obtain new access tokens without requiring the user to log in again, thus improving user experience and security by limiting the lifespan of access tokens.

Authorization: Bearer <token>
 Backend Authentication: Access & Refresh Tokens, Middleware, and Cookies 



   Access & Refresh Tokens 
-  Access Token : Short-lived, used to authorize access to protected resources.
-  Refresh Token : Long-lived, used to obtain a new access token without requiring re-login.
- Enhances security and user experience.
- Upon access token expiration, the refresh token generates a new one.

---

   Building the Login Functionality 
1.  Retrieve data  from the request body (email/username & password).
2.  Validate input : Ensure username or email is provided.
3.  Find user  in the database using `$or` operator in MongoDB.
4.  Handle errors : Return a 404 error if the user is not found.
5.  Verify password  using `isPasswordCorrect` (likely `bcrypt`).
6.  Generate tokens : Create access and refresh tokens using model methods.
7.  Store refresh token  in the database (`validateBeforeSave: false`).
8.  Send tokens securely  via HTTP-only cookies.
9.  Return JSON response  including user data and tokens.

---

   Custom Middleware for Authentication 
- Middleware functions execute during request/response cycles.
-  `verifyJWT` middleware: 
  - Extracts the token from cookies or the Authorization header.
  - If from Authorization header, removes "Bearer " prefix.
  - Verifies the token using `jsonwebtoken.verify`.
  - Decodes token to retrieve user ID.
  - Finds user in the database, excluding password & refresh token.
  - If valid, attaches the user object to `req.user` and calls `next()`.
- Wrapped in a `try...catch` block for error handling.

---

   Logout Functionality 
1.  Clear HTTP-only cookies  using `res.clearCookie()`.
2.  Invalidate refresh token  in the database (`findByIdAndUpdate`).
3.  Ensure authentication : `verifyJWT` middleware protects the `/logout` route.
4.  Send JSON response  confirming successful logout.

---

   Routing & Middleware Integration 
- Express Router defines routes.
- Middleware functions (e.g., `verifyJWT`) protect specific routes.
- Middleware order matters; `next()` ensures proper execution flow.

---

   Best Practices 
-  Descriptive method names  for readability (e.g., `generateAccessTokenAndRefreshToken`).
-  Use `async...await`  for handling asynchronous operations.
-  Error handling  via `try...catch` and custom error classes.
-  Optimize Mongoose queries  using `.select()` to exclude unnecessary fields.
-  Enhance security  with `httpOnly` and `secure` cookie flags.
-  Follow production practices , such as using `_` for unused parameters.

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                    Purpose of Tokens:

Access and refresh tokens eliminate the need for users to repeatedly enter credentials to access protected resources.

Access Token:
Short-lived: Valid for a limited time (e.g., one hour, one day).
Becomes invalid after expiration.

Refresh Token:
Longer lifespan than access tokens.
Stored in the database securely.
Used to obtain new access tokens when the existing one expires.

Workflow for Refreshing Tokens:

Frontend receives a 401 Unauthorized error due to an expired access token.
Sends a POST request to the refresh token endpoint, including the refresh token.
Backend verifies the refresh token against the stored token in the database.
Uses jsonwebtoken.verify() with a secret key for validation.
If valid, generates a new access token and a new refresh token.
Sends the new tokens back to the frontend as HTTP-only and secure cookies.
If verification fails (token not found, expired, or mismatched), backend returns a 401 error.

Backend Implementation Details:

A refreshAccessToken controller function is created to handle token renewal.
Extracts refresh token from cookies or request body.
Queries the database to find the user linked to the refresh token.
Validates and matches the incoming refresh token with the stored one.
Uses generateAccessAndRefreshTokens() to create new tokens.
Sends the new tokens via secure, HTTP-only cookies and a JSON response.
Implements error handling using a try-catch block.
Defines a route (/api/v1/users/refresh-token) using a POST method.

Key Takeaway:

Access tokens provide short-term authentication.
Refresh tokens allow new access tokens to be obtained without re-login, enhancing security and user experience.

Debugging Tips:

Use console logs to track data flow and detect issues.
Review Git commit history to trace code changes.
Check import statements (including file extensions like .js if required).
Verify conditional logic and ensure all necessary validation steps are in place.
Step-by-step debugging is crucial for backend development.

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                     Subscription 
           Writing Update Controllers for User Accounts in a Backend Application

It focuses on writing and modifying update controllers for user accounts, specifically the edit functionality. By the end of this video, viewers will be able to efficiently handle most user-related controllers.

Subscription Model Implementation
 a new Subscription model is introduced:
Separate from the User model to follow best practices.

Includes two fields:
subscriber – References the user who subscribes.
channel – References the user being subscribed to.

Uses timestamps (createdAt, updatedAt) for tracking changes.
The model is committed with the message "Add subscription model".

Controllers Implemented
1. changeCurrentPassword
Allows the logged-in user to update their password.
Verifies the old password before updating.

Uses bcrypt to hash the new password.
2. getCurrentUser
Retrieves and returns the currently logged-in user’s details.
Uses middleware to populate req.user.

3. updateAccountDetails
Allows users to update their full name and email.
Uses MongoDB’s $set operator to modify fields.
Excludes the password field from the response for security.

4. updateUserAvatar
Handles avatar image uploads using Cloudinary.
Validates the uploaded file before processing.
Updates the avatar field in the database.

5. updateUserCoverImage
Similar to updateUserAvatar, but for the cover image.

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////                                      MongoDB Aggregation Pipelines
$match
$project
$lookup

db.orders.aggregate ([{},{}]) -> array and object where each object is a storage
[
   {

//firstpipeline
$lookup:{            //->which document to join ?
from:
localField:
foreignField:
as: // this as is in array 
   }
 },

 //secondpipeline
 $addFields:{
   author_details:{

   }

 }
]

1. Aggregation Pipelines
A series of stages that process documents step by step.
Each stage transforms the data and passes the output to the next stage.
Executed using db.collection.aggregate() with an array of stages.

2. Key Aggregation Stages
🔹 $match – Filtering Documents
Similar to SQL’s WHERE clause.
Used to find specific documents based on given criteria.

🔹 $lookup – Performing Joins
Used for left joins between collections.
Requires:

from: Collection to join with.
localField: Field from the input documents.
foreignField: Field from the joined collection.
as: Name of the resulting array field.
Example: Joining users with subscribers and subscriptions.

 $addFields – Adding/Modifying Fields
Adds new fields or modifies existing ones.
Useful for:
Extracting the first element from an array (from $lookup).
Calculating counts using $size.
Using $cond (conditional logic) and $in to check values.
Example: Creating subscribersCount, channelsSubscribedToCount, and isSubscribed.

🔹 $project – Reshaping Data
Controls which fields appear in the output.
Helps reduce network traffic by only returning necessary fields.
3. Practical Example: Fetching a User's Channel Profile
API Endpoint: /api/v1/users/:username
Steps:
Extract username from the URL.
Use $match to find the user.
Use $lookup to:
Fetch the user's subscribers.
Fetch the channels the user has subscribed to.
Use $addFields to compute counts and determine subscription status.
Use $project to return only relevant fields like username, email, avatar, etc.

4. Handling Output & Errors
Aggregation pipelines return arrays of documents, even if only one document matches.
In application logic, access the first element of the resulting array.
Check if the username exists before processing.

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                              How to write sub pipelines and routes
                          Advanced Aggregation Pipelines & User Routes
Key Data Points
Each video contains owner, description, duration, views, published date, and creation date.
The watchHistory field in the user document stores an array of video IDs.

Steps in the Aggregation Pipeline
1 Joining Watch History with Videos ($lookup)
The watchHistory array (local field) is matched with _id (foreign field) in the videos collection.
This retrieves multiple video documents in an array.

2.Nested $lookup for Owner Details
Since each video has an owner field (user ID), a nested $lookup joins it with the users collection.
This fetches owner details inside the video document.

3.Refining the Nested Lookup Output ($project)
The nested lookup initially retrieves all user data, which is excessive.
To optimize, $project is used inside the sub-pipeline to include only necessary fields (e.g., username, avatar).

4.Handling Array Output of $lookup ($addFields)
Since $lookup always returns an array, the owner field will contain an array with one user object.
To extract the single owner object, $addFields is used with $first or $arrayElemAt.
This simplifies frontend access (video.owner.username instead of video.owner[0].username).

Writing the Get Watch History Handler
Key Considerations
MongoDB ObjectId Handling in Aggregation
In Mongoose, req.user._id is a string, but MongoDB requires an ObjectId.
Mongoose automatically converts it when using findById, but not in aggregation pipelines.
Solution: Convert manually using:
new mongoose.Types.ObjectId(req.user._id)

Pipeline Structure
The pipeline begins with a $match stage to filter watch history by user ID.
The result is formatted to return only the watch history instead of the full user object.

User Routes in Express
Key Routes & Handlers

Method	    Path	                             Middleware	                                   Handler
POST	   /api/v1/users/change-password	        verifyJWT	                             changeCurrentPassword
GET	     /api/v1/users/current-user	        verifyJWT	                                  getCurrentUser
PATCH	     /api/v1/users/update-account	     verifyJWT	                               updateAccountDetails
PATCH	   /api/v1/users/avatar	verifyJWT,     upload.single("avatar")	                     updateUserAvatar
PATCH	    /api/v1/users/cover-image	        verifyJWT, upload.single("coverImage")	     updateUserCoverImage
GET	     /api/v1/users/c/:username	        verifyJWT	                              getUserChannelProfile
GET	       /api/v1/users/history	            verifyJWT	                               getWatchHistory

Route Highlights 
File Upload Handling
multer middleware handles single-file uploads for avatar and cover images.

Dynamic Route Parameters
The :username parameter in /c/:username allows fetching user profiles by their username.

Security Considerations
All routes require authentication (verifyJWT) to ensure only authorized users can access them.

Readability & Maintainability
Routes are structured for clarity and scalability, and can be adjusted as needed.
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                       Summary of Backend Series -
1 Connecting Frontend & Backend
Handling CORS errors and setting up proxies to manage cross-origin resource sharing.
Understanding how APIs interact with the frontend.

2.HTTP Crash Course
Explains HTTP methods (GET, POST, PUT, DELETE) and their role in backend development.
Understanding how requests and responses work, including status codes and headers.

4.Introduction to Express.js and Mongoose
Express.js is used for routing and handling requests efficiently.
Mongoose is introduced for database interaction with MongoDB.

Major Backend Concepts Implemented
  1. Environment Variables
From the very first video, emphasis was placed on using .env files to manage sensitive data like database credentials and API keys.

 2.Minimal Package Usage
Only essential packages were used, such as:

bcrypt, jsonwebtoken (for authentication)
cloudinary, multer (for file handling)
cookie-parser, dotenv, mongoose, etc.
The goal was to understand the core concepts first before relying on external libraries.

 3.Database Connection (MongoDB & Mongoose)
The series goes beyond simple mongoose.connect(), exploring:
Connecting databases in different geographical regions.
Handling database connection errors efficiently.
Listening to connection events for debugging.

 4.App Structure & Routing
Setting up app.js/index.js to configure the backend properly.
Professional routing setup, including API versioning (/api/v1/users).
Organized routes in a structured and scalable manner

5. Express.js Features & Middleware
Rate limiting for security.
Cookie parsing to handle user authentication.
URL encoding for smooth request handling.
Middleware usage for authentication and logging.

 6.User Model & Authentication
User model implementation with:
Mongoose Hooks (pre-save for password hashing).
Custom model methods (for JWT token generation).
 
 User authentication covered:
Registering users (Multer for file uploads, Cloudinary for storage).
Login and logout functionalities.
Token management (refresh tokens & access tokens).

7. CRUD Operations on User Data
Creating, reading, updating, and deleting users using Mongoose functions:
find, findById, findOneAndUpdate, findOneAndDelete.
Change password functionality was implemented as an "update" operation.

 8. Management (JWT)
Refresh tokens vs. Access tokens explained in detail.
Implementing secure user sessions using JWT.
Tokens were stored securely and refreshed periodically.

 9.File Handling with Multer & Cloudinary
File upload, update, and deletion using Multer (middleware).
Storing images and videos on Cloudinary.
Updating file metadata in MongoDB.

10.Video & Subscription Models
Video model implementation (for user-generated content).
Subscription model (advanced and complex logic).
Complex aggregation pipelines used to fetch subscriptions and video data efficiently.

11.Aggregation Pipelines & Advanced Queries
Used $lookup, $addFields, $project to:
Fetch user watch history.
Retrieve subscription details dynamically.
Manage complex data joins between multiple collections.

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                             Postman
-It is easily to test in route 



